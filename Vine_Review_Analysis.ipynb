{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEOhx1yj87Rp",
    "outputId": "5a06c591-7a6b-4db0-c44a-cebfb2b87ac3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop2.7.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.1.3-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10648/1860159231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    161\u001b[0m             raise Exception(\n\u001b[0;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m--> 163\u001b[1;33m                     \u001b[0mspark_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m                 )\n\u001b[0;32m    165\u001b[0m             )\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.3-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.1.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bn56Msgg9u00"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11900/3448758925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjTrywi191RR",
    "outputId": "56afa216-2bf9-4514-9935-81ff429051a6"
   },
   "outputs": [],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.25.jre7.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFEVjA2Y95fB"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Amazon Reviews ETL').config('spark.driver.extraClassPath', '/content/postgresql-42.2.25.jre7.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r7051rV96SQ"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz'\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(''), sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM3uwppb9mwY"
   },
   "outputs": [],
   "source": [
    "twenty_or_more_df = df.filter(df[\"total_votes\"] >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prMIzwBE9JrE"
   },
   "outputs": [],
   "source": [
    "fifty_percent_df = twenty_or_more_df.filter(twenty_or_more_df['helpful_votes'] / twenty_or_more_df['total_votes'] >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2MzpvzR9N0d"
   },
   "outputs": [],
   "source": [
    "paid_df = fifty_percent_df.filter(fifty_percent_df['vine'] == 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgyZ4Amx9R-X"
   },
   "outputs": [],
   "source": [
    "not_paid_df = fifty_percent_df.filter(fifty_percent_df['vine'] == 'N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xj1paO99UVE",
    "outputId": "aa0cb75a-60f7-43e8-de2f-b227f5650f29"
   },
   "outputs": [],
   "source": [
    "fifty_percent_df.show()\n",
    "twenty_or_more_df.show()\n",
    "paid_df.show()\n",
    "not_paid_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfV9muzk-oue"
   },
   "outputs": [],
   "source": [
    "paid_five_stars = paid_df.select('star_rating').where(paid_df.star_rating == 5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDmbSy7h-_9n",
    "outputId": "c26ea912-bf89-48e5-9390-20ab1780c7b1"
   },
   "outputs": [],
   "source": [
    "paid_five_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmaosYsj_EhY"
   },
   "outputs": [],
   "source": [
    "not_paid_five_stars = not_paid_df.select('star_rating').where(not_paid_df.star_rating ==5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSoaxN1-_PGG",
    "outputId": "117d600d-0cde-402c-97f9-00e41b412054"
   },
   "outputs": [],
   "source": [
    "not_paid_five_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US76fIX6_2fh"
   },
   "outputs": [],
   "source": [
    "paid_star_percentage = paid_df.select('star_rating').where(paid_df.star_rating == 5).count() / paid_df.select('star_rating').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fI6f88DBbev",
    "outputId": "2bba4c00-bc6e-469f-b032-8291748a4af3"
   },
   "outputs": [],
   "source": [
    "paid_star_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxqvVF26CH3I"
   },
   "outputs": [],
   "source": [
    "not_paid_star_percentage = not_paid_df.select('star_rating').where(not_paid_df.star_rating == 5).count() / not_paid_df.select('star_rating').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BhN7gfvCSSM",
    "outputId": "570a2e06-65b8-4808-baa3-2a1f1ced33dd"
   },
   "outputs": [],
   "source": [
    "not_paid_star_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mQRMbNtCTmT"
   },
   "outputs": [],
   "source": [
    "vine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vine_Review_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
